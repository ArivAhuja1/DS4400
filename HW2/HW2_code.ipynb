{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Linear Regression\n",
    "## Ariv Ahuja\n",
    "## DS4400 - Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('housing/train.csv', index_col=0)\n",
    "test_df = pd.read_csv('housing/test.csv', index_col=0)\n",
    "\n",
    "# Drop columns: id, date, zipcode (as per instructions)\n",
    "cols_to_drop = ['id', 'date', 'zipcode']\n",
    "for col in cols_to_drop:\n",
    "    if col in train_df.columns:\n",
    "        train_df = train_df.drop(columns=[col])\n",
    "    if col in test_df.columns:\n",
    "        test_df = test_df.drop(columns=[col])\n",
    "\n",
    "# Separate features and target\n",
    "y_train = train_df['price'].values / 1000  # Divide by 1000 as per instructions\n",
    "y_test = test_df['price'].values / 1000\n",
    "\n",
    "X_train = train_df.drop(columns=['price'])\n",
    "X_test = test_df.drop(columns=['price'])\n",
    "\n",
    "# Store feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Scale features to mean 0 and std 1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train_scaled.shape[0]} samples, {X_train_scaled.shape[1]} features\")\n",
    "print(f\"Testing set size: {X_test_scaled.shape[0]} samples\")\n",
    "print(f\"\\nFeatures: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Linear Regression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Train multiple linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get coefficients\n",
    "print(\"=\" * 60)\n",
    "print(\"Problem 2.1: Linear Regression Coefficients\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nIntercept (theta_0): {lr_model.intercept_:.4f}\")\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "for name, coef in zip(feature_names, lr_model.coef_):\n",
    "    print(f\"  {name:20s}: {coef:12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "y_train_pred = lr_model.predict(X_train_scaled)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"  MSE: {train_mse:.4f}\")\n",
    "print(f\"  R^2: {train_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Evaluate on testing set\n",
    "y_test_pred = lr_model.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Problem 2.2: Testing Metrics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  MSE: {test_mse:.4f}\")\n",
    "print(f\"  R^2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Analysis - Top contributing features\n",
    "print(\"=\" * 60)\n",
    "print(\"Problem 2.3: Feature Importance Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort features by absolute coefficient value\n",
    "feature_importance = sorted(zip(feature_names, lr_model.coef_), \n",
    "                           key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features (by absolute coefficient):\")\n",
    "for i, (name, coef) in enumerate(feature_importance[:5], 1):\n",
    "    print(f\"  {i}. {name:20s}: {coef:12.4f}\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Training MSE: {train_mse:.4f}, Testing MSE: {test_mse:.4f}\")\n",
    "print(f\"  Training R^2: {train_r2:.4f}, Testing R^2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Closed-Form Solution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionClosedForm:\n",
    "    \"\"\"Linear Regression using closed-form solution: theta = (X^T X)^(-1) X^T y\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using closed-form solution with pseudo-inverse for numerical stability.\"\"\"\n",
    "        # Add bias column (column of ones)\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Closed-form solution using pseudo-inverse: theta = (X^T X)^(-1) X^T y\n",
    "        # Using pinv for numerical stability\n",
    "        self.theta = np.linalg.pinv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the trained model.\"\"\"\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.theta\n",
    "    \n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        return self.theta[0]\n",
    "    \n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.theta[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using closed-form solution\n",
    "cf_model = LinearRegressionClosedForm()\n",
    "cf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Problem 3: Closed-Form Solution Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nIntercept (theta_0): {cf_model.intercept_:.4f}\")\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "for name, coef in zip(feature_names, cf_model.coef_):\n",
    "    print(f\"  {name:20s}: {coef:12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate closed-form model\n",
    "y_train_pred_cf = cf_model.predict(X_train_scaled)\n",
    "y_test_pred_cf = cf_model.predict(X_test_scaled)\n",
    "\n",
    "cf_train_mse = mean_squared_error(y_train, y_train_pred_cf)\n",
    "cf_train_r2 = r2_score(y_train, y_train_pred_cf)\n",
    "cf_test_mse = mean_squared_error(y_test, y_test_pred_cf)\n",
    "cf_test_r2 = r2_score(y_test, y_test_pred_cf)\n",
    "\n",
    "print(\"\\nClosed-Form Implementation Metrics:\")\n",
    "print(f\"  Training MSE: {cf_train_mse:.4f}, R^2: {cf_train_r2:.4f}\")\n",
    "print(f\"  Testing MSE:  {cf_test_mse:.4f}, R^2: {cf_test_r2:.4f}\")\n",
    "\n",
    "print(\"\\nComparison with sklearn:\")\n",
    "print(f\"  sklearn    - Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}\")\n",
    "print(f\"  Closed-form - Train MSE: {cf_train_mse:.4f}, Test MSE: {cf_test_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X, degree):\n",
    "    \"\"\"Create polynomial features up to specified degree.\"\"\"\n",
    "    X = np.array(X).reshape(-1, 1) if X.ndim == 1 else X\n",
    "    features = [X]\n",
    "    for d in range(2, degree + 1):\n",
    "        features.append(X ** d)\n",
    "    return np.hstack(features)\n",
    "\n",
    "# Extract sqft_living feature\n",
    "sqft_living_idx = feature_names.index('sqft_living')\n",
    "X_sqft_train = X_train_scaled[:, sqft_living_idx]\n",
    "X_sqft_test = X_test_scaled[:, sqft_living_idx]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Problem 4: Polynomial Regression Results\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nUsing feature: sqft_living\")\n",
    "\n",
    "results_poly = []\n",
    "for p in [1, 2, 3, 4, 5]:\n",
    "    # Create polynomial features\n",
    "    X_poly_train = create_polynomial_features(X_sqft_train, p)\n",
    "    X_poly_test = create_polynomial_features(X_sqft_test, p)\n",
    "    \n",
    "    # Fit model using closed-form solution\n",
    "    poly_model = LinearRegressionClosedForm()\n",
    "    poly_model.fit(X_poly_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred_poly = poly_model.predict(X_poly_train)\n",
    "    y_test_pred_poly = poly_model.predict(X_poly_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_mse_poly = mean_squared_error(y_train, y_train_pred_poly)\n",
    "    train_r2_poly = r2_score(y_train, y_train_pred_poly)\n",
    "    test_mse_poly = mean_squared_error(y_test, y_test_pred_poly)\n",
    "    test_r2_poly = r2_score(y_test, y_test_pred_poly)\n",
    "    \n",
    "    results_poly.append({\n",
    "        'Degree': p,\n",
    "        'Train MSE': train_mse_poly,\n",
    "        'Train R^2': train_r2_poly,\n",
    "        'Test MSE': test_mse_poly,\n",
    "        'Test R^2': test_r2_poly\n",
    "    })\n",
    "\n",
    "# Display results table\n",
    "results_df = pd.DataFrame(results_poly)\n",
    "print(\"\\nPolynomial Regression Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    \"\"\"Linear Regression using Gradient Descent.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using gradient descent.\"\"\"\n",
    "        # Add bias column\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        n_samples, n_features = X_b.shape\n",
    "        \n",
    "        # Initialize theta to zeros\n",
    "        self.theta = np.zeros(n_features)\n",
    "        self.cost_history = []\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            predictions = X_b @ self.theta\n",
    "            \n",
    "            # Compute gradients\n",
    "            errors = predictions - y\n",
    "            gradients = (2 / n_samples) * (X_b.T @ errors)\n",
    "            \n",
    "            # Update theta\n",
    "            self.theta -= self.learning_rate * gradients\n",
    "            \n",
    "            # Store cost (MSE)\n",
    "            cost = np.mean(errors ** 2)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the trained model.\"\"\"\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.theta\n",
    "    \n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        return self.theta[0]\n",
    "    \n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.theta[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Problem 5: Gradient Descent Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "iterations_list = [10, 50, 100]\n",
    "\n",
    "results_gd = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for n_iter in iterations_list:\n",
    "        gd_model = LinearRegressionGD(learning_rate=lr, n_iterations=n_iter)\n",
    "        gd_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred_gd = gd_model.predict(X_train_scaled)\n",
    "        y_test_pred_gd = gd_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        train_mse_gd = mean_squared_error(y_train, y_train_pred_gd)\n",
    "        train_r2_gd = r2_score(y_train, y_train_pred_gd)\n",
    "        test_mse_gd = mean_squared_error(y_test, y_test_pred_gd)\n",
    "        test_r2_gd = r2_score(y_test, y_test_pred_gd)\n",
    "        \n",
    "        results_gd.append({\n",
    "            'Learning Rate': lr,\n",
    "            'Iterations': n_iter,\n",
    "            'Train MSE': train_mse_gd,\n",
    "            'Train R^2': train_r2_gd,\n",
    "            'Test MSE': test_mse_gd,\n",
    "            'Test R^2': test_r2_gd\n",
    "        })\n",
    "\n",
    "# Display results table\n",
    "results_gd_df = pd.DataFrame(results_gd)\n",
    "print(\"\\nGradient Descent Results:\")\n",
    "print(results_gd_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence for different learning rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "for lr in [0.01, 0.1]:\n",
    "    gd_model = LinearRegressionGD(learning_rate=lr, n_iterations=100)\n",
    "    gd_model.fit(X_train_scaled, y_train)\n",
    "    plt.plot(gd_model.cost_history, label=f'LR={lr}')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE (Cost)')\n",
    "plt.title('Gradient Descent Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('gd_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6: Ridge Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegressionGD:\n",
    "    \"\"\"Ridge Regression using Gradient Descent.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, lambda_reg=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model using gradient descent with L2 regularization.\"\"\"\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        n_samples, n_features = X_b.shape\n",
    "        \n",
    "        self.theta = np.zeros(n_features)\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            predictions = X_b @ self.theta\n",
    "            errors = predictions - y\n",
    "            \n",
    "            # Gradient with regularization (don't regularize bias term)\n",
    "            gradients = (2 / n_samples) * (X_b.T @ errors)\n",
    "            gradients[1:] += (2 * self.lambda_reg / n_samples) * self.theta[1:]\n",
    "            \n",
    "            self.theta -= self.learning_rate * gradients\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.theta\n",
    "    \n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        return self.theta[0]\n",
    "    \n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.theta[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 6.3: Simulated data experiment\n",
    "print(\"=\" * 60)\n",
    "print(\"Problem 6.3: Ridge Regression on Simulated Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 1000\n",
    "\n",
    "# Generate X uniformly on [-2, 2]\n",
    "X_sim = np.random.uniform(-2, 2, N)\n",
    "\n",
    "# Generate Y = 1 + 2*X + e, where e ~ N(0, 2)\n",
    "e = np.random.normal(0, np.sqrt(2), N)\n",
    "Y_sim = 1 + 2 * X_sim + e\n",
    "\n",
    "# Reshape for model fitting\n",
    "X_sim_reshaped = X_sim.reshape(-1, 1)\n",
    "\n",
    "# Linear regression (no regularization) using closed-form\n",
    "X_sim_b = np.c_[np.ones((N, 1)), X_sim_reshaped]\n",
    "theta_lr = np.linalg.pinv(X_sim_b.T @ X_sim_b) @ X_sim_b.T @ Y_sim\n",
    "y_pred_lr = X_sim_b @ theta_lr\n",
    "\n",
    "print(f\"\\nLinear Regression (lambda=0):\")\n",
    "print(f\"  Intercept: {theta_lr[0]:.4f} (true: 1.0)\")\n",
    "print(f\"  Slope: {theta_lr[1]:.4f} (true: 2.0)\")\n",
    "print(f\"  MSE: {mean_squared_error(Y_sim, y_pred_lr):.4f}\")\n",
    "print(f\"  R^2: {r2_score(Y_sim, y_pred_lr):.4f}\")\n",
    "\n",
    "# Ridge regression with different lambda values using closed-form\n",
    "lambdas = [1, 10, 100, 1000, 10000]\n",
    "results_ridge = []\n",
    "\n",
    "print(\"\\nRidge Regression Results:\")\n",
    "print(f\"{'Lambda':<12} {'Intercept':<12} {'Slope':<12} {'MSE':<12} {'R^2':<12}\")\n",
    "print('-'*60)\n",
    "\n",
    "for lam in lambdas:\n",
    "    # Closed-form ridge: theta = (X^T X + lambda*I)^(-1) X^T y\n",
    "    I = np.eye(X_sim_b.shape[1])\n",
    "    I[0, 0] = 0  # Don't regularize bias\n",
    "    theta_ridge = np.linalg.pinv(X_sim_b.T @ X_sim_b + lam * I) @ X_sim_b.T @ Y_sim\n",
    "    y_pred_ridge = X_sim_b @ theta_ridge\n",
    "    \n",
    "    mse_ridge = mean_squared_error(Y_sim, y_pred_ridge)\n",
    "    r2_ridge = r2_score(Y_sim, y_pred_ridge)\n",
    "    \n",
    "    print(f\"{lam:<12} {theta_ridge[0]:<12.4f} {theta_ridge[1]:<12.4f} {mse_ridge:<12.4f} {r2_ridge:<12.4f}\")\n",
    "    \n",
    "    results_ridge.append({\n",
    "        'Lambda': lam,\n",
    "        'Intercept': theta_ridge[0],\n",
    "        'Slope': theta_ridge[1],\n",
    "        'MSE': mse_ridge,\n",
    "        'R^2': r2_ridge\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CODE EXECUTION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
